{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "* Autoencoders are unsupervised learning models used to learn representations.\n",
    "\n",
    "* It is composed by two parts: the encoder and the decoder.\n",
    "\n",
    "* The encoder is responsible for creating a representation of the original input (code, feature embedding) and the decoder is responsible for trying to recreate the original input from this code. Both encoder and decoder can be built from dense layers.\n",
    "\n",
    "* The decoder output is not the same as the original input as it was transformed by the encoder and not all the information from it is necessarily present in the code. \n",
    "\n",
    "* Loss function: as the loss is computed comparing the input to the decoder output, regression related loss functions are used to train the autoencoder. We can use, for example, the mean squared error.\n",
    "\n",
    "* After training the autoencoder, it is possible to use the representation (code) for a series of applications. It can be used for indexing data, datasearch in large datasets, and classification useing other ML method.\n",
    "\n",
    "## Undercomplete autoencoders\n",
    "\n",
    "* An undercomplete autoencoder's encoder obtains representations that have a reduced dimensionality when compared to the input. They can be used to learn a dimensionality reduction.\n",
    "\n",
    "* A dense autoencoder with a single encoder/decoder layer has mathematical relation to the PCA method: if the data manifold is linear, the autoencoder tends to converge to a projection of the $m$ principal components of the data.\n",
    "\n",
    "* Hence, this autoencoder makes a compression of the input, with information loss and the code related layer is called \"bottloneck\".\n",
    "\n",
    "* The autoencoder learns to recreate, to its best capabilities, the original inputs. So it is trained to learn patterns on the code that relate to a certain feature in the original input.\n",
    "\n",
    "* If the autoencoder is trained to recreate images of numbers from 0 to 9, for example, and then is feeded with images of clothes, for example, the output will try to find patterns that ressembles a number on the clothing image. Hence, ir recreates clothes images in ressemblance to numbers.\n",
    "\n",
    "* Deep undercomplete autoencoders: they can have other types of layers like convolutioinal layers and pooling layers. The code layer is ini most cases a fully conected (dense) layer to allow the projection of the data.\n",
    "\n",
    "## Overcomplete autoencoders\n",
    "\n",
    "* Intermediary layer (code layer) with bigger or equal dimensionality than the input.\n",
    "\n",
    "* A simple implementation would learn to \"copy\" the input, as the code layer has this capacity. Hence, it is necessary to restrain this space.\n",
    "\n",
    "* To prevent this, we can use regularization or dropout techiniques.\n",
    "\n",
    "## Denoising autoencoders\n",
    "\n",
    "* This type of autoencoder, that can be under or overcomplete, is trainet  to remove noise from an input. The input of this neural network corresponds to the original input with some noise added and the loss function is calculated between the final output and the original input.\n",
    "\n",
    "* This architecture allows the neural network to learn to remove noise from the original input.\n",
    "\n",
    "* This noise can be introduced by adding some noise (like a gaussian noise) or remove some points (replace by zero) on the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
