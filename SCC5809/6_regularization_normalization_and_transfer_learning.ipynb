{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization, normalization and transfer learning\n",
    "\n",
    "* In Neural Networks applications, sometimes it is not about the most novel architectures, but about the techniques used to improve the NN.\n",
    "\n",
    "* Sometimes, consolidated architectures can have better performance than novel ones if regularization, normalization and transfer learning are used.\n",
    "\n",
    "### Trainning deep neural netowrks in real world scenarios\n",
    "\n",
    "* Machine learning can be formulated as a parameter learning task of a given function $f: X \\rightarrow Y$. Hence, a ML algorithm fits $f$ considering a space $\\mathbb{F}$ of possible functions. The quantity of possible functions in $\\mathbb{F}$ has impact in the learning task. If there are more degrees of freedom, there is a possibility of overffiting a function to the data. On the other hand, less degrees of freedom can lead to underffiting.\n",
    "\n",
    "* A restricted $\\mathbb{F}$ space will have a strong bias and a broad one will have a weak bias.\n",
    "\n",
    "* Approximation error vs estimation error: an approximation error is related to the convergence of the learned function to the local optima (considering a space $\\mathbb{F}$). An estimation error is related to the impossibility to converge to an optima becasuse it is not in the learning space $\\mathbb{F}$. This is directly related to the **variance bias tradeoff**.\n",
    "\n",
    "![Alt text](images/variance_bias_tradeoff.png)\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Overparametrization\n",
    "\n",
    "![Alt text](images/overparametrization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### Regulatization\n",
    "\n",
    "This process aims to prevent the model to \"overspecialize\" on the data. This procedure penalizes the model for fitting large weights values. It can be performed by using a penalty for each layer or by using a global penalty for the entire network.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Randomly deactivates neurons with a probability $p$ for each iteration, limitating the capacity of certain parameters to memorize the data.\n",
    "\n",
    "### Early stop\n",
    "\n",
    "This method aims to stop the training process before the model overffits. To achieve this, we can use a validation data set and stop the training process depending on the relation between the performance on the training and in the validation sets.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "In data augmentation, the objective is to generate artificial data points with the aim to improve the model convergence. For structured data, there are techniques such as SMOTE and for images, rotation, sliceing, noise insertion, and other procedures can be made.\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "Transfer learning consists of using a neural network that was trained on another dataset for another learning task, taking advantage of the fact thet the neural network is not randomly initialized, but trained on real data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
