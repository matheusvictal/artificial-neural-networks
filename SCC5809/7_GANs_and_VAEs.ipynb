{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gererator Models and Variational Autoencoders (VAEs)\n",
    "\n",
    "A generator model will gererate outputs based on previous learning of desired patterns. A genrator model can be used, for example, to generate images of faces baased on previous image patterns that it was exposed to.\n",
    "\n",
    "This type of model learns the data probability distribution and generates data from the learned distribution via sampling. Considering not only neural networks, there are two classes of methods for this purpose: the ones that learn the density function explicitly and the ones that learn it implicity.\n",
    "\n",
    "Explicit density function:\n",
    "* Fullly visible belief nets;\n",
    "* Boltzmann Machines;\n",
    "* Variational Autoencoders.\n",
    "\n",
    "Implicit densoty function:\n",
    "* Monte Carlo methods;\n",
    "* Likelihood-free inference via classification.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Variational Autoencoders (VAEs)\n",
    "\n",
    "Autoencoders are used to learn to codify latent variables (the ones that can not be directly observed) into a variable. A VAE learns probability distributions (its parameters) of each variable from which samples can be taken. In other words, the codified latend variables are the probability function parameters. For this task, a distribution needs to be defined a priori, so we can learn its parameters.\n",
    "\n",
    "![Alt text](images/vae.png)\n",
    "\n",
    "The loss function used in VAEs is the Evidence Lower Bound (ELBO). This method consists of the attribution to the loss function a reconstruction loss (MES/Cross-entropy) added with the KL divergence of the learned distribution (encoder output) with a $N(0,1)$ distribution.\n",
    "\n",
    "Obs.: \"Disentanglement\" is the field of study that tries to separate attributtes in a way we can interpret them.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Generative Adversarial Networks (GANs)\n",
    "\n",
    "The basic idea of this model is the competition between two learning components with the main objective of generation data. The two components are:\n",
    "\n",
    "* Generator $G$: Receives a random input and generates an output from the domain of interest.\n",
    "* Discriminator $D$: Receives the output of a generator and a domain example and decides if the generated data came from the original distribution or from the generator.\n",
    "\n",
    "Both components compete in a zero sum game, in other words, if one component wins, the other loses.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Diffusion-based models\n",
    "\n",
    "The diffusion process is characterized for the addition of noise to the data until the complete loss of its original information. A model that can revert the diffussion procedure is a diffusion-based method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
