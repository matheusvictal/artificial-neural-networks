{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to neural networks optimization\n",
    "\n",
    "* The complexity of the problem indicates how deep a neural network has to be to succeed. But with deep neural networks comes optimization problems.\n",
    "\n",
    "* Gradient descent: a way to avoid brute forcing the way through optmization, which is not viable, speccialy for complex scenarios. The operation is made in the cost function surface. The original algorithm is supcetible to getting locked in a local minima and never reach the true minima of the surface.\n",
    "\n",
    "* In a scenario that the parameters are not continuous and well behaved, the gradient descent method may fall short, since it is a method based on the mathematical definition of the gradient. \n",
    "\n",
    "* Optimization libraries/methods: gridsearch, hyperopt, optuna, NAS (Neural Architecture Search).\n",
    "\n",
    "* A computational graph is used to store all the results from the calculus involved in the backpropgation procedure, in other words, the chain rule calculus results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch perceptron implementation:\n",
    "\n",
    "The following code corresponds to the implementation of a regressor perceptron in pytorch. Here, it is given enphasis to the feed foward and gradient descent backpropagation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    # constructor: receives self, number of inputs (num_imputs) and a learning rate value (lr)\n",
    "    def __init__(self, num_inputs, lr=.01):\n",
    "        # w -> sampled from a normal distribution with mean 0 and standard deviation 1\n",
    "        self.w = torch.normal(mean=0, std=1, size=(num_inputs,1), requires_grad=True)\n",
    "        # b -> constant value (1 or 0) or even 1/total number of classes\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "        # class attribute to store inputs' number\n",
    "        self.num_inputs = num_inputs\n",
    "        # class attribute to store the learning rate value\n",
    "        self.lr = lr  \n",
    "\n",
    "    # Define a ReLU activation function\n",
    "    def activation_relu(self, x):\n",
    "        a = torch.zeros_like(x)\n",
    "        return torch.max(x, a)\n",
    "\n",
    "    # foward -> processes an input X value through the output\n",
    "    def foward(self, X):\n",
    "        linear = X@self.w + self.b\n",
    "        return self.activation_relu(linear)\n",
    "    \n",
    "    # backward -> obtains the prediction error running back the layer\n",
    "    def backward(self, X, y):\n",
    "        y_hat = self.foward(X) # foward pass\n",
    "        errors = (y.reshape(y_hat.shape) - y_hat) # differentiate\n",
    "        return errors\n",
    "\n",
    "    # quadratic loss\n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y.reshape(y_hat.shape) - y_hat)**2/2\n",
    "        return l.mean()\n",
    "    \n",
    "    # train\n",
    "    def train_step(self, X, y):\n",
    "        for i in range(y.shape[0]):\n",
    "            error = self.backward(X[i].reshape(1, self.num_inputs), y[i]).reshape(-1) # transforms the unitary tensor to a scalar value\n",
    "            # gradient descent\n",
    "            self.w = self.w + self.lr*error*X[i].reshape(self.num_inputs, 1)\n",
    "            self.b = self.b + self.lr*error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.1306],\n",
       "         [1.6895],\n",
       "         [1.8499]], requires_grad=True),\n",
       " tensor([0.], requires_grad=True),\n",
       " 0.001)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Perceptron(3, lr=0.001)\n",
    "model.w, model.b, model.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8904,  6.7750,  0.6223],\n",
      "        [ 1.4300,  4.4493,  5.1651],\n",
      "        [ 3.2495,  3.9364,  9.0632],\n",
      "        [11.2349,  8.8244, 11.9049],\n",
      "        [11.8409, 16.5702, 17.1188],\n",
      "        [12.9469, 17.1069, 13.7759],\n",
      "        [16.4468, 19.7310, 19.8193],\n",
      "        [20.7280, 22.3385, 21.6008],\n",
      "        [24.9066, 22.9328, 26.6466],\n",
      "        [28.6048, 29.0486, 29.3416]])\n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Generating example data\n",
    "X = torch.arange(30, dtype=torch.float32).reshape((10,3)) + torch.normal(0, 2, (10,3))\n",
    "print(X)\n",
    "y = torch.arange(10, dtype=torch.float32)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 14.7349],\n",
       "         [ 18.6889],\n",
       "         [ 27.0908],\n",
       "         [ 49.6344],\n",
       "         [ 73.0513],\n",
       "         [ 69.0243],\n",
       "         [ 88.5946],\n",
       "         [101.1361],\n",
       "         [116.1990],\n",
       "         [135.6982]], grad_fn=<MaximumBackward0>),\n",
       " tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n",
       " tensor(2782.2212, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.foward(X), y, model.loss(model.foward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: tensor([[-0.0887],\n",
      "        [ 0.1888],\n",
      "        [ 0.1955]], grad_fn=<AddBackward0>) tensor([-0.1467], grad_fn=<AddBackward0>)\n",
      "Squared loss:, 0.224431\n"
     ]
    }
   ],
   "source": [
    "model.train_step(X, y)\n",
    "print('Parameters:', model.w, model.b)\n",
    "print(f'Squared loss:, {model.loss(model.foward(X), y).item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: [[-0.08677812]\n",
      " [ 0.18390043]\n",
      " [ 0.1922458 ]] -0.1477540284395218\n",
      "Squared loss:, 0.243705\n",
      "Parameters: [[-0.08296393]\n",
      " [ 0.18144888]\n",
      " [ 0.19160113]] -0.14859594404697418\n",
      "Squared loss:, 0.237253\n",
      "Parameters: [[-0.07922538]\n",
      " [ 0.17907315]\n",
      " [ 0.19094062]] -0.14942799508571625\n",
      "Squared loss:, 0.231081\n",
      "Parameters: [[-0.07556455]\n",
      " [ 0.17676663]\n",
      " [ 0.19026239]] -0.15025046467781067\n",
      "Squared loss:, 0.225222\n",
      "Parameters: [[-0.07198013]\n",
      " [ 0.17452696]\n",
      " [ 0.1895689 ]] -0.1510632187128067\n",
      "Squared loss:, 0.219657\n",
      "Parameters: [[-0.06847092]\n",
      " [ 0.17235175]\n",
      " [ 0.18886238]] -0.15186618268489838\n",
      "Squared loss:, 0.214367\n",
      "Parameters: [[-0.06503566]\n",
      " [ 0.17023873]\n",
      " [ 0.1881449 ]] -0.1526593267917633\n",
      "Squared loss:, 0.209335\n",
      "Parameters: [[-0.06167302]\n",
      " [ 0.16818589]\n",
      " [ 0.18741846]] -0.1534426063299179\n",
      "Squared loss:, 0.204546\n",
      "Parameters: [[-0.05838189]\n",
      " [ 0.16619098]\n",
      " [ 0.18668461]] -0.1542159914970398\n",
      "Squared loss:, 0.199988\n",
      "Parameters: [[-0.05516097]\n",
      " [ 0.16425216]\n",
      " [ 0.18594512]] -0.15497945249080658\n",
      "Squared loss:, 0.195645\n",
      "Parameters: [[-0.05200894]\n",
      " [ 0.16236761]\n",
      " [ 0.18520154]] -0.15573303401470184\n",
      "Squared loss:, 0.191505\n",
      "Parameters: [[-0.04892467]\n",
      " [ 0.16053538]\n",
      " [ 0.18445516]] -0.15647673606872559\n",
      "Squared loss:, 0.187558\n",
      "Parameters: [[-0.04590683]\n",
      " [ 0.1587539 ]\n",
      " [ 0.18370737]] -0.1572105884552002\n",
      "Squared loss:, 0.183792\n",
      "Parameters: [[-0.04295429]\n",
      " [ 0.15702139]\n",
      " [ 0.18295923]] -0.15793465077877045\n",
      "Squared loss:, 0.180197\n",
      "Parameters: [[-0.04006576]\n",
      " [ 0.15533637]\n",
      " [ 0.1822119 ]] -0.15864896774291992\n",
      "Squared loss:, 0.176765\n",
      "Parameters: [[-0.03724013]\n",
      " [ 0.1536972 ]\n",
      " [ 0.18146628]] -0.1593535840511322\n",
      "Squared loss:, 0.173487\n",
      "Parameters: [[-0.03447616]\n",
      " [ 0.15210247]\n",
      " [ 0.18072335]] -0.16004860401153564\n",
      "Squared loss:, 0.170354\n",
      "Parameters: [[-0.03177267]\n",
      " [ 0.15055081]\n",
      " [ 0.17998394]] -0.16073405742645264\n",
      "Squared loss:, 0.167359\n",
      "Parameters: [[-0.0291285 ]\n",
      " [ 0.14904083]\n",
      " [ 0.17924881]] -0.16141006350517273\n",
      "Squared loss:, 0.164495\n",
      "Parameters: [[-0.02654254]\n",
      " [ 0.14757122]\n",
      " [ 0.1785186 ]] -0.1620766967535019\n",
      "Squared loss:, 0.161756\n",
      "Parameters: [[-0.02401353]\n",
      " [ 0.14614084]\n",
      " [ 0.17779404]] -0.1627340465784073\n",
      "Squared loss:, 0.159134\n",
      "Parameters: [[-0.02154046]\n",
      " [ 0.14474837]\n",
      " [ 0.17707556]] -0.16338220238685608\n",
      "Squared loss:, 0.156624\n",
      "Parameters: [[-0.01912226]\n",
      " [ 0.14339265]\n",
      " [ 0.1763637 ]] -0.164021298289299\n",
      "Squared loss:, 0.154222\n",
      "Parameters: [[-0.01675768]\n",
      " [ 0.14207268]\n",
      " [ 0.17565905]] -0.16465143859386444\n",
      "Squared loss:, 0.151920\n",
      "Parameters: [[-0.01444576]\n",
      " [ 0.1407873 ]\n",
      " [ 0.17496188]] -0.16527274250984192\n",
      "Squared loss:, 0.149714\n",
      "Parameters: [[-0.01218536]\n",
      " [ 0.13953553]\n",
      " [ 0.17427264]] -0.16588528454303741\n",
      "Squared loss:, 0.147600\n",
      "Parameters: [[-0.0099755 ]\n",
      " [ 0.13831633]\n",
      " [ 0.1735916 ]] -0.16648918390274048\n",
      "Squared loss:, 0.145574\n",
      "Parameters: [[-0.00781514]\n",
      " [ 0.13712877]\n",
      " [ 0.17291905]] -0.16708457469940186\n",
      "Squared loss:, 0.143631\n",
      "Parameters: [[-0.00570316]\n",
      " [ 0.135972  ]\n",
      " [ 0.17225534]] -0.1676715761423111\n",
      "Squared loss:, 0.141767\n",
      "Parameters: [[-0.00363867]\n",
      " [ 0.13484503]\n",
      " [ 0.17160058]] -0.16825029253959656\n",
      "Squared loss:, 0.139979\n",
      "Parameters: [[-0.00162068]\n",
      " [ 0.13374703]\n",
      " [ 0.17095497]] -0.16882085800170898\n",
      "Squared loss:, 0.138263\n",
      "Parameters: [[0.00035182]\n",
      " [0.1326772 ]\n",
      " [0.17031871]] -0.16938342154026031\n",
      "Squared loss:, 0.136616\n",
      "Parameters: [[0.00227972]\n",
      " [0.13163467]\n",
      " [0.16969192]] -0.1699380725622177\n",
      "Squared loss:, 0.135035\n",
      "Parameters: [[0.0041641 ]\n",
      " [0.13061881]\n",
      " [0.16907479]] -0.17048494517803192\n",
      "Squared loss:, 0.133516\n",
      "Parameters: [[0.0060057 ]\n",
      " [0.1296287 ]\n",
      " [0.16846725]] -0.1710241734981537\n",
      "Squared loss:, 0.132058\n",
      "Parameters: [[0.00780552]\n",
      " [0.12866372]\n",
      " [0.16786948]] -0.17155587673187256\n",
      "Squared loss:, 0.130656\n",
      "Parameters: [[0.00956447]\n",
      " [0.12772322]\n",
      " [0.16728152]] -0.1720801442861557\n",
      "Squared loss:, 0.129309\n",
      "Parameters: [[0.01128335]\n",
      " [0.12680641]\n",
      " [0.16670334]] -0.17259715497493744\n",
      "Squared loss:, 0.128015\n",
      "Parameters: [[0.01296311]\n",
      " [0.12591277]\n",
      " [0.16613509]] -0.17310699820518494\n",
      "Squared loss:, 0.126770\n",
      "Parameters: [[0.01460449]\n",
      " [0.12504154]\n",
      " [0.1655766 ]] -0.17360979318618774\n",
      "Squared loss:, 0.125574\n",
      "Parameters: [[0.0162084 ]\n",
      " [0.12419222]\n",
      " [0.165028  ]] -0.1741056889295578\n",
      "Squared loss:, 0.124423\n",
      "Parameters: [[0.0177756 ]\n",
      " [0.12336417]\n",
      " [0.16448916]] -0.17459477484226227\n",
      "Squared loss:, 0.123316\n",
      "Parameters: [[0.01930686]\n",
      " [0.12255675]\n",
      " [0.16396005]] -0.1750771850347519\n",
      "Squared loss:, 0.122250\n",
      "Parameters: [[0.02080303]\n",
      " [0.12176953]\n",
      " [0.16344072]] -0.17555305361747742\n",
      "Squared loss:, 0.121225\n",
      "Parameters: [[0.02226482]\n",
      " [0.12100191]\n",
      " [0.16293104]] -0.1760224848985672\n",
      "Squared loss:, 0.120239\n",
      "Parameters: [[0.02369299]\n",
      " [0.12025338]\n",
      " [0.16243094]] -0.17648561298847198\n",
      "Squared loss:, 0.119289\n",
      "Parameters: [[0.02508829]\n",
      " [0.11952345]\n",
      " [0.16194037]] -0.17694254219532013\n",
      "Squared loss:, 0.118374\n",
      "Parameters: [[0.02645145]\n",
      " [0.11881164]\n",
      " [0.16145925]] -0.17739340662956238\n",
      "Squared loss:, 0.117493\n",
      "Parameters: [[0.02778313]\n",
      " [0.11811742]\n",
      " [0.16098744]] -0.1778382956981659\n",
      "Squared loss:, 0.116645\n",
      "Parameters: [[0.02908401]\n",
      " [0.11744035]\n",
      " [0.16052485]] -0.17827732861042023\n",
      "Squared loss:, 0.115828\n",
      "Parameters: [[0.03035489]\n",
      " [0.11678009]\n",
      " [0.16007149]] -0.17871059477329254\n",
      "Squared loss:, 0.115040\n",
      "Parameters: [[0.0315963 ]\n",
      " [0.11613607]\n",
      " [0.15962715]] -0.1791382133960724\n",
      "Squared loss:, 0.114281\n",
      "Parameters: [[0.03280898]\n",
      " [0.11550799]\n",
      " [0.1591918 ]] -0.17956028878688812\n",
      "Squared loss:, 0.113549\n",
      "Parameters: [[0.0339935 ]\n",
      " [0.11489534]\n",
      " [0.15876526]] -0.1799769550561905\n",
      "Squared loss:, 0.112843\n",
      "Parameters: [[0.03515049]\n",
      " [0.11429774]\n",
      " [0.15834743]] -0.18038831651210785\n",
      "Squared loss:, 0.112163\n",
      "Parameters: [[0.03628063]\n",
      " [0.11371489]\n",
      " [0.15793821]] -0.18079446256160736\n",
      "Squared loss:, 0.111507\n",
      "Parameters: [[0.03738445]\n",
      " [0.11314631]\n",
      " [0.15753749]] -0.1811954826116562\n",
      "Squared loss:, 0.110874\n",
      "Parameters: [[0.03846255]\n",
      " [0.11259171]\n",
      " [0.15714513]] -0.18159149587154388\n",
      "Squared loss:, 0.110264\n",
      "Parameters: [[0.03951558]\n",
      " [0.11205076]\n",
      " [0.15676105]] -0.18198256194591522\n",
      "Squared loss:, 0.109674\n",
      "Parameters: [[0.04054394]\n",
      " [0.11152299]\n",
      " [0.15638502]] -0.18236882984638214\n",
      "Squared loss:, 0.109106\n",
      "Parameters: [[0.04154835]\n",
      " [0.11100824]\n",
      " [0.15601704]] -0.1827503740787506\n",
      "Squared loss:, 0.108557\n",
      "Parameters: [[0.04252917]\n",
      " [0.11050601]\n",
      " [0.15565683]] -0.1831272691488266\n",
      "Squared loss:, 0.108027\n",
      "Parameters: [[0.04348709]\n",
      " [0.11001611]\n",
      " [0.15530442]] -0.18349964916706085\n",
      "Squared loss:, 0.107516\n",
      "Parameters: [[0.04442251]\n",
      " [0.10953815]\n",
      " [0.15495956]] -0.18386757373809814\n",
      "Squared loss:, 0.107022\n",
      "Parameters: [[0.04533605]\n",
      " [0.10907197]\n",
      " [0.15462226]] -0.18423113226890564\n",
      "Squared loss:, 0.106545\n",
      "Parameters: [[0.04622811]\n",
      " [0.10861715]\n",
      " [0.15429226]] -0.1845904439687729\n",
      "Squared loss:, 0.106085\n",
      "Parameters: [[0.04709927]\n",
      " [0.10817353]\n",
      " [0.15396959]] -0.18494555354118347\n",
      "Squared loss:, 0.105639\n",
      "Parameters: [[0.04794988]\n",
      " [0.1077407 ]\n",
      " [0.15365392]] -0.18529656529426575\n",
      "Squared loss:, 0.105209\n",
      "Parameters: [[0.04878044]\n",
      " [0.10731845]\n",
      " [0.15334521]] -0.1856435388326645\n",
      "Squared loss:, 0.104793\n",
      "Parameters: [[0.04959146]\n",
      " [0.10690653]\n",
      " [0.1530434 ]] -0.18598657846450806\n",
      "Squared loss:, 0.104391\n",
      "Parameters: [[0.05038331]\n",
      " [0.10650466]\n",
      " [0.1527483 ]] -0.18632575869560242\n",
      "Squared loss:, 0.104003\n",
      "Parameters: [[0.05115646]\n",
      " [0.10611264]\n",
      " [0.15245979]] -0.18666115403175354\n",
      "Squared loss:, 0.103628\n",
      "Parameters: [[0.05191131]\n",
      " [0.10573016]\n",
      " [0.15217774]] -0.1869928240776062\n",
      "Squared loss:, 0.103265\n",
      "Parameters: [[0.05264837]\n",
      " [0.10535713]\n",
      " [0.15190214]] -0.18732085824012756\n",
      "Squared loss:, 0.102913\n",
      "Parameters: [[0.05336791]\n",
      " [0.10499316]\n",
      " [0.15163271]] -0.1876453459262848\n",
      "Squared loss:, 0.102574\n",
      "Parameters: [[0.05407043]\n",
      " [0.10463813]\n",
      " [0.15136942]] -0.18796634674072266\n",
      "Squared loss:, 0.102245\n",
      "Parameters: [[0.05475624]\n",
      " [0.10429177]\n",
      " [0.15111211]] -0.18828395009040833\n",
      "Squared loss:, 0.101927\n",
      "Parameters: [[0.05542573]\n",
      " [0.10395388]\n",
      " [0.15086062]] -0.18859820067882538\n",
      "Squared loss:, 0.101620\n",
      "Parameters: [[0.0560793 ]\n",
      " [0.10362426]\n",
      " [0.15061492]] -0.18890917301177979\n",
      "Squared loss:, 0.101322\n",
      "Parameters: [[0.05671735]\n",
      " [0.10330278]\n",
      " [0.15037495]] -0.1892169564962387\n",
      "Squared loss:, 0.101034\n",
      "Parameters: [[0.05734013]\n",
      " [0.1029891 ]\n",
      " [0.1501404 ]] -0.18952156603336334\n",
      "Squared loss:, 0.100755\n",
      "Parameters: [[0.05794807]\n",
      " [0.10268317]\n",
      " [0.14991133]] -0.18982312083244324\n",
      "Squared loss:, 0.100485\n",
      "Parameters: [[0.05854145]\n",
      " [0.10238472]\n",
      " [0.14968753]] -0.1901216208934784\n",
      "Squared loss:, 0.100224\n",
      "Parameters: [[0.05912071]\n",
      " [0.10209366]\n",
      " [0.14946902]] -0.19041715562343597\n",
      "Squared loss:, 0.099971\n",
      "Parameters: [[0.05968606]\n",
      " [0.10180972]\n",
      " [0.14925559]] -0.19070981442928314\n",
      "Squared loss:, 0.099726\n",
      "Parameters: [[0.06023785]\n",
      " [0.10153276]\n",
      " [0.14904712]] -0.1909996122121811\n",
      "Squared loss:, 0.099488\n",
      "Parameters: [[0.06077637]\n",
      " [0.10126259]\n",
      " [0.14884354]] -0.19128663837909698\n",
      "Squared loss:, 0.099258\n",
      "Parameters: [[0.06130201]\n",
      " [0.10099915]\n",
      " [0.14864482]] -0.1915709376335144\n",
      "Squared loss:, 0.099035\n",
      "Parameters: [[0.06181493]\n",
      " [0.10074213]\n",
      " [0.14845067]] -0.19185256958007812\n",
      "Squared loss:, 0.098820\n",
      "Parameters: [[0.06231551]\n",
      " [0.10049147]\n",
      " [0.14826119]] -0.19213159382343292\n",
      "Squared loss:, 0.098611\n",
      "Parameters: [[0.06280404]\n",
      " [0.10024704]\n",
      " [0.14807624]] -0.19240805506706238\n",
      "Squared loss:, 0.098408\n",
      "Parameters: [[0.06328076]\n",
      " [0.10000863]\n",
      " [0.14789566]] -0.19268198311328888\n",
      "Squared loss:, 0.098211\n",
      "Parameters: [[0.06374595]\n",
      " [0.09977612]\n",
      " [0.14771937]] -0.1929534524679184\n",
      "Squared loss:, 0.098021\n",
      "Parameters: [[0.06419986]\n",
      " [0.09954936]\n",
      " [0.14754733]] -0.19322249293327332\n",
      "Squared loss:, 0.097836\n",
      "Parameters: [[0.06464279]\n",
      " [0.09932826]\n",
      " [0.14737941]] -0.193489208817482\n",
      "Squared loss:, 0.097657\n",
      "Parameters: [[0.06507496]\n",
      " [0.09911263]\n",
      " [0.1472155 ]] -0.19375355541706085\n",
      "Squared loss:, 0.097484\n",
      "Parameters: [[0.06549666]\n",
      " [0.09890241]\n",
      " [0.14705558]] -0.19401566684246063\n",
      "Squared loss:, 0.097316\n",
      "Parameters: [[0.06590804]\n",
      " [0.09869738]\n",
      " [0.14689949]] -0.19427555799484253\n",
      "Squared loss:, 0.097152\n",
      "Parameters: [[0.06630936]\n",
      " [0.09849742]\n",
      " [0.14674716]] -0.19453327357769012\n",
      "Squared loss:, 0.096994\n",
      "Parameters: [[0.06670094]\n",
      " [0.09830253]\n",
      " [0.1465986 ]] -0.1947888433933258\n",
      "Squared loss:, 0.096841\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for _ in range(epochs):\n",
    "    model.train_step(X, y)\n",
    "    print('Parameters:', model.w.detach().numpy(), model.b.item())\n",
    "    print(f'Squared loss:, {model.loss(model.foward(X), y).item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6885],\n",
       "         [1.0952],\n",
       "         [1.7376],\n",
       "         [3.1673],\n",
       "         [4.7335],\n",
       "         [4.3700],\n",
       "         [5.7473],\n",
       "         [6.5504],\n",
       "         [7.6272],\n",
       "         [8.8702]], grad_fn=<MaximumBackward0>),\n",
       " tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.foward(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9571,  3.3450,  2.8526],\n",
      "        [ 1.8246,  3.6858,  6.0044],\n",
      "        [ 4.3695,  5.0810, 11.6046],\n",
      "        [ 9.7113, 10.4942, 13.4864],\n",
      "        [10.1406, 14.9435, 15.0168]])\n",
      "tensor([0., 1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "# Test:\n",
    "# Generating example data\n",
    "X = torch.arange(15, dtype=torch.float32).reshape((5,3)) + torch.normal(0, 2, (5,3))\n",
    "print(X)\n",
    "y = torch.arange(5, dtype=torch.float32)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4884],\n",
       "         [1.1695],\n",
       "         [2.2974],\n",
       "         [3.4617],\n",
       "         [4.1520]], grad_fn=<MaximumBackward0>),\n",
       " tensor([0., 1., 2., 3., 4.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.foward(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0592, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss(model.foward(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd example in pytorch\n",
    "\n",
    "[torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) is PyTorchâ€™s automatic differentiation engine that powers neural network trainin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.5000], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.])\n",
    "w = torch.tensor([1.2], requires_grad=True)\n",
    "b = torch.tensor([0.5], requires_grad=True)\n",
    "\n",
    "act = F.relu(w*x + b)\n",
    "print(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.]),)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(act, w, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing dense neural networks (MLP) with pytorch: layers, gradient and optmizers\n",
    "\n",
    "* Pytorch pre-implemented layers\n",
    "* Parameter access options\n",
    "* Gradient use\n",
    "* Backpropagation and and optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the ```foward``` and ```backward``` functions (where the gradients are computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Calls the __init__ dunder method (constructor) from the superclass nn.Module\n",
    "        super(Network, self).__init__()\n",
    "        # Creates the network project\n",
    "        # The problem of interest is an image classification problem (handwritten numbers from 0 to 9)\n",
    "        # 28x28 pixels images = 724 values\n",
    "        # Network output = 10 classes\n",
    "        self.fc1 = nn.Linear(784, 32) # fully conected layer (first dense layer)\n",
    "        self.fc2 = nn.Linear(32, 10) # fully conected layer (second dense layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is an image 28x28 -> does not work for the conneccted layer, which expects a 1d vector\n",
    "        # Receives minibatches\n",
    "        X = torch.flatten(X, 1) # flatten the dimentions with the exception of the batches one\n",
    "        X = F.relu(self.fc1(X)) # connected linear layer + relu\n",
    "        X = self.fc2(X) # connected linear layer\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "tensor([[ 0.0847,  0.1732,  0.1617, -0.0896,  0.0639, -0.2462, -0.0805, -0.1427,\n",
      "         -0.1804,  0.0198]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_random = torch.randn(1, 1, 28, 28)\n",
    "print(input_random.shape)\n",
    "output = net(input_random)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3431, -0.2508,  0.2336,  0.3299, -0.1642,  0.0559,  0.0749, -0.1185,\n",
      "          0.1411,  0.2383]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input_random) # executes the foward method\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Parameter containing:\n",
      "tensor([[-0.0242,  0.0085, -0.0324,  ...,  0.0316,  0.0316,  0.0337],\n",
      "        [ 0.0020, -0.0196, -0.0106,  ..., -0.0182,  0.0049,  0.0075],\n",
      "        [ 0.0319,  0.0310,  0.0191,  ...,  0.0058,  0.0356,  0.0270],\n",
      "        ...,\n",
      "        [-0.0209,  0.0206, -0.0283,  ..., -0.0212,  0.0146, -0.0142],\n",
      "        [ 0.0266, -0.0299, -0.0311,  ..., -0.0237, -0.0317,  0.0179],\n",
      "        [-0.0087, -0.0064,  0.0016,  ...,  0.0048, -0.0063, -0.0286]],\n",
      "       requires_grad=True)\n",
      "torch.Size([32, 784])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "# first layer parameters\n",
    "print(params[0])\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "#initialize gradients buffer\n",
    "net.zero_grad()\n",
    "output.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7661,  0.6755, -0.0827,  1.9172, -1.2256,  0.2290, -0.0954,  0.7112,\n",
      "         1.1470, -1.3312])\n",
      "tensor([[-0.7661,  0.6755, -0.0827,  1.9172, -1.2256,  0.2290, -0.0954,  0.7112,\n",
      "          1.1470, -1.3312]])\n",
      "tensor(1.1268, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input_random)\n",
    "target = torch.randn(10) # random target just for example purposes\n",
    "print(target)\n",
    "target = target.view(1,-1)\n",
    "print(target)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x000001672599BE20>\n",
      "<AddmmBackward0 object at 0x0000016734144B20>\n",
      "<AccumulateGrad object at 0x0000016734145960>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0]) # linear function\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antes\n",
      "None\n",
      "depois\n",
      "tensor([ 0.0033,  0.0217,  0.0000,  0.0000, -0.0581,  0.0713,  0.0346,  0.0000,\n",
      "         0.0440,  0.0000,  0.1349,  0.0000, -0.0099,  0.1201,  0.0000,  0.1233,\n",
      "        -0.0656,  0.0000,  0.0000,  0.0000,  0.0598, -0.0395,  0.0000,  0.0000,\n",
      "         0.0093,  0.0000,  0.0000,  0.0376,  0.0000, -0.0149,  0.0000,  0.0079])\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "net.zero_grad()\n",
    "\n",
    "print('antes')\n",
    "print(net.fc1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "print('depois')\n",
    "print(net.fc1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0242,  0.0085, -0.0324, -0.0352, -0.0321,  0.0334,  0.0240,  0.0271,\n",
      "         0.0334,  0.0179], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(params[0][0][:10])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# creates the optimizer object\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "\n",
    "# for each training loop\n",
    "\n",
    "## initialize gradient buffer\n",
    "optimizer.zero_grad()\n",
    "## generate output and compute gradients with relation to the loss function\n",
    "output = net(input_random)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "## wieght adaptation\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0239,  0.0083, -0.0323, -0.0352, -0.0321,  0.0329,  0.0236,  0.0276,\n",
      "         0.0337,  0.0185], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(params[0][0][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
